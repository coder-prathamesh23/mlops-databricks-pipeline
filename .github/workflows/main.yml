name: Databricks CI/CD Pipeline

on:
  push:
    branches:
      - main  # Trigger this workflow on pushes to the main branch

jobs:
  deploy-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Unit Tests
        run: PYTHONPATH=. pytest

      - name: Deploy and Run on Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          # Install the NEW, UNIFIED Databricks CLI
          curl -fsSL https://docs.databricks.com/en/dev-tools/cli/install.sh | sh

          # The following commands use the new CLI's syntax
          databricks fs cp src/pipeline.py dbfs:/pipelines/pyspark_pipeline.py --overwrite

          # Submit the job using the new CLI's 'jobs submit' command
          databricks jobs submit --json '{
            "name": "CICD_Run_PySpark_Pipeline",
            "new_cluster": {
              "spark_version": "13.3.x-scala2.12",
              "node_type_id": "i3.xlarge",
              "num_workers": 1,
              "aws_attributes": {
                "instance_profile_arn": "arn:aws:iam::511194961172:instance-profile/DatabricksS3Role"
              }
            },
            "spark_python_task": {
              "python_file": "dbfs:/pipelines/pyspark_pipeline.py"
            }
          }'